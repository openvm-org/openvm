use std::{
    array,
    borrow::{Borrow, BorrowMut},
};

use openvm_circuit::{
    arch::{
        execution_mode::E1E2ExecutionCtx, AdapterAirContext, AdapterTraceStep, ExecuteFunc,
        MinimalInstruction, PreComputeInstruction, Result, StepExecutorE1, TraceStep,
        VmAdapterInterface, VmCoreAir, VmSegmentState, VmStateMut,
    },
    next_instruction,
    system::memory::{online::TracingMemory, MemoryAuxColsFactory},
};
use openvm_circuit_primitives::{
    bitwise_op_lookup::{BitwiseOperationLookupBus, SharedBitwiseOperationLookupChip},
    range_tuple::{RangeTupleCheckerBus, SharedRangeTupleCheckerChip},
};
use openvm_circuit_primitives_derive::{AlignedBorrow, AlignedBytesBorrow};
use openvm_instructions::{
    instruction::Instruction,
    program::DEFAULT_PC_STEP,
    riscv::{RV32_REGISTER_AS, RV32_REGISTER_NUM_LIMBS},
    LocalOpcode,
};
use openvm_rv32im_transpiler::MulHOpcode;
use openvm_stark_backend::{
    interaction::InteractionBuilder,
    p3_air::{AirBuilder, BaseAir},
    p3_field::{Field, FieldAlgebra, PrimeField32},
    rap::BaseAirWithPublicValues,
};
use strum::IntoEnumIterator;

#[repr(C)]
#[derive(AlignedBorrow)]
pub struct MulHCoreCols<T, const NUM_LIMBS: usize, const LIMB_BITS: usize> {
    pub a: [T; NUM_LIMBS],
    pub b: [T; NUM_LIMBS],
    pub c: [T; NUM_LIMBS],

    pub a_mul: [T; NUM_LIMBS],
    pub b_ext: T,
    pub c_ext: T,

    pub opcode_mulh_flag: T,
    pub opcode_mulhsu_flag: T,
    pub opcode_mulhu_flag: T,
}

#[derive(Copy, Clone, Debug, derive_new::new)]
pub struct MulHCoreAir<const NUM_LIMBS: usize, const LIMB_BITS: usize> {
    pub bitwise_lookup_bus: BitwiseOperationLookupBus,
    pub range_tuple_bus: RangeTupleCheckerBus<2>,
}

impl<F: Field, const NUM_LIMBS: usize, const LIMB_BITS: usize> BaseAir<F>
    for MulHCoreAir<NUM_LIMBS, LIMB_BITS>
{
    fn width(&self) -> usize {
        MulHCoreCols::<F, NUM_LIMBS, LIMB_BITS>::width()
    }
}
impl<F: Field, const NUM_LIMBS: usize, const LIMB_BITS: usize> BaseAirWithPublicValues<F>
    for MulHCoreAir<NUM_LIMBS, LIMB_BITS>
{
}

impl<AB, I, const NUM_LIMBS: usize, const LIMB_BITS: usize> VmCoreAir<AB, I>
    for MulHCoreAir<NUM_LIMBS, LIMB_BITS>
where
    AB: InteractionBuilder,
    I: VmAdapterInterface<AB::Expr>,
    I::Reads: From<[[AB::Expr; NUM_LIMBS]; 2]>,
    I::Writes: From<[[AB::Expr; NUM_LIMBS]; 1]>,
    I::ProcessedInstruction: From<MinimalInstruction<AB::Expr>>,
{
    fn eval(
        &self,
        builder: &mut AB,
        local_core: &[AB::Var],
        _from_pc: AB::Var,
    ) -> AdapterAirContext<AB::Expr, I> {
        let cols: &MulHCoreCols<_, NUM_LIMBS, LIMB_BITS> = local_core.borrow();
        let flags = [
            cols.opcode_mulh_flag,
            cols.opcode_mulhsu_flag,
            cols.opcode_mulhu_flag,
        ];

        let is_valid = flags.iter().fold(AB::Expr::ZERO, |acc, &flag| {
            builder.assert_bool(flag);
            acc + flag.into()
        });
        builder.assert_bool(is_valid.clone());

        let b = &cols.b;
        let c = &cols.c;
        let carry_divide = AB::F::from_canonical_u32(1 << LIMB_BITS).inverse();

        // Note b * c = a << LIMB_BITS + a_mul, in order to constrain that a is correct we
        // need to compute the carries generated by a_mul.
        let a_mul = &cols.a_mul;
        let mut carry_mul: [AB::Expr; NUM_LIMBS] = array::from_fn(|_| AB::Expr::ZERO);

        for i in 0..NUM_LIMBS {
            let expected_limb = if i == 0 {
                AB::Expr::ZERO
            } else {
                carry_mul[i - 1].clone()
            } + (0..=i).fold(AB::Expr::ZERO, |ac, k| ac + (b[k] * c[i - k]));
            carry_mul[i] = AB::Expr::from(carry_divide) * (expected_limb - a_mul[i]);
        }

        for (a_mul, carry_mul) in a_mul.iter().zip(carry_mul.iter()) {
            self.range_tuple_bus
                .send(vec![(*a_mul).into(), carry_mul.clone()])
                .eval(builder, is_valid.clone());
        }

        // We can now constrain that a is correct using carry_mul[NUM_LIMBS - 1]
        let a = &cols.a;
        let mut carry: [AB::Expr; NUM_LIMBS] = array::from_fn(|_| AB::Expr::ZERO);

        for j in 0..NUM_LIMBS {
            let expected_limb = if j == 0 {
                carry_mul[NUM_LIMBS - 1].clone()
            } else {
                carry[j - 1].clone()
            } + ((j + 1)..NUM_LIMBS)
                .fold(AB::Expr::ZERO, |acc, k| acc + (b[k] * c[NUM_LIMBS + j - k]))
                + (0..(j + 1)).fold(AB::Expr::ZERO, |acc, k| {
                    acc + (b[k] * cols.c_ext) + (c[k] * cols.b_ext)
                });
            carry[j] = AB::Expr::from(carry_divide) * (expected_limb - a[j]);
        }

        for (a, carry) in a.iter().zip(carry.iter()) {
            self.range_tuple_bus
                .send(vec![(*a).into(), carry.clone()])
                .eval(builder, is_valid.clone());
        }

        // Check that b_ext and c_ext are correct using bitwise lookup. We check
        // both b and c when the opcode is MULH, and only b when MULHSU.
        let sign_mask = AB::F::from_canonical_u32(1 << (LIMB_BITS - 1));
        let ext_inv = AB::F::from_canonical_u32((1 << LIMB_BITS) - 1).inverse();
        let b_sign = cols.b_ext * ext_inv;
        let c_sign = cols.c_ext * ext_inv;

        builder.assert_bool(b_sign.clone());
        builder.assert_bool(c_sign.clone());
        builder
            .when(cols.opcode_mulhu_flag)
            .assert_zero(b_sign.clone());
        builder
            .when(cols.opcode_mulhu_flag + cols.opcode_mulhsu_flag)
            .assert_zero(c_sign.clone());

        self.bitwise_lookup_bus
            .send_range(
                AB::Expr::from_canonical_u32(2) * (b[NUM_LIMBS - 1] - b_sign * sign_mask),
                (cols.opcode_mulh_flag + AB::Expr::ONE) * (c[NUM_LIMBS - 1] - c_sign * sign_mask),
            )
            .eval(builder, cols.opcode_mulh_flag + cols.opcode_mulhsu_flag);

        let expected_opcode = VmCoreAir::<AB, I>::expr_to_global_expr(
            self,
            flags.iter().zip(MulHOpcode::iter()).fold(
                AB::Expr::ZERO,
                |acc, (flag, local_opcode)| {
                    acc + (*flag).into() * AB::Expr::from_canonical_u8(local_opcode as u8)
                },
            ),
        );

        AdapterAirContext {
            to_pc: None,
            reads: [cols.b.map(Into::into), cols.c.map(Into::into)].into(),
            writes: [cols.a.map(Into::into)].into(),
            instruction: MinimalInstruction {
                is_valid,
                opcode: expected_opcode,
            }
            .into(),
        }
    }

    fn start_offset(&self) -> usize {
        MulHOpcode::CLASS_OFFSET
    }
}

pub struct MulHStep<A, const NUM_LIMBS: usize, const LIMB_BITS: usize> {
    adapter: A,
    pub bitwise_lookup_chip: SharedBitwiseOperationLookupChip<LIMB_BITS>,
    pub range_tuple_chip: SharedRangeTupleCheckerChip<2>,
}

impl<A, const NUM_LIMBS: usize, const LIMB_BITS: usize> MulHStep<A, NUM_LIMBS, LIMB_BITS> {
    pub fn new(
        adapter: A,
        bitwise_lookup_chip: SharedBitwiseOperationLookupChip<LIMB_BITS>,
        range_tuple_chip: SharedRangeTupleCheckerChip<2>,
    ) -> Self {
        // The RangeTupleChecker is used to range check (a[i], carry[i]) pairs where 0 <= i
        // < 2 * NUM_LIMBS. a[i] must have LIMB_BITS bits and carry[i] is the sum of i + 1
        // bytes (with LIMB_BITS bits). BitwiseOperationLookup is used to sign check bytes.
        debug_assert!(
            range_tuple_chip.sizes()[0] == 1 << LIMB_BITS,
            "First element of RangeTupleChecker must have size {}",
            1 << LIMB_BITS
        );
        debug_assert!(
            range_tuple_chip.sizes()[1] >= (1 << LIMB_BITS) * 2 * NUM_LIMBS as u32,
            "Second element of RangeTupleChecker must have size of at least {}",
            (1 << LIMB_BITS) * 2 * NUM_LIMBS as u32
        );

        Self {
            adapter,
            bitwise_lookup_chip,
            range_tuple_chip,
        }
    }
}

impl<F, CTX, A, const NUM_LIMBS: usize, const LIMB_BITS: usize> TraceStep<F, CTX>
    for MulHStep<A, NUM_LIMBS, LIMB_BITS>
where
    F: PrimeField32,
    A: 'static
        + for<'a> AdapterTraceStep<
            F,
            CTX,
            ReadData: Into<[[u8; NUM_LIMBS]; 2]>,
            WriteData: From<[[u8; NUM_LIMBS]; 1]>,
            TraceContext<'a> = (),
        >,
{
    fn get_opcode_name(&self, opcode: usize) -> String {
        format!(
            "{:?}",
            MulHOpcode::from_usize(opcode - MulHOpcode::CLASS_OFFSET)
        )
    }

    fn execute(
        &mut self,
        state: VmStateMut<F, TracingMemory<F>, CTX>,
        instruction: &Instruction<F>,
        trace: &mut [F],
        trace_offset: &mut usize,
        width: usize,
    ) -> Result<()> {
        let Instruction { opcode, .. } = instruction;

        let mulh_opcode = MulHOpcode::from_usize(opcode.local_opcode_idx(MulHOpcode::CLASS_OFFSET));

        let row_slice = &mut trace[*trace_offset..*trace_offset + width];
        let (adapter_row, core_row) = unsafe { row_slice.split_at_mut_unchecked(A::WIDTH) };

        A::start(*state.pc, state.memory, adapter_row);

        let [rs1, rs2] = self
            .adapter
            .read(state.memory, instruction, adapter_row)
            .into();

        let b = rs1.map(u32::from);
        let c = rs2.map(u32::from);
        let (a, a_mul, carry, b_ext, c_ext) = run_mulh::<NUM_LIMBS, LIMB_BITS>(mulh_opcode, &b, &c);

        let core_row: &mut MulHCoreCols<_, NUM_LIMBS, LIMB_BITS> = core_row.borrow_mut();
        core_row.a = a.map(F::from_canonical_u32);
        core_row.b = b.map(F::from_canonical_u32);
        core_row.c = c.map(F::from_canonical_u32);
        core_row.a_mul = a_mul.map(F::from_canonical_u32);
        core_row.b_ext = F::from_canonical_u32(b_ext);
        core_row.c_ext = F::from_canonical_u32(c_ext);
        core_row.opcode_mulh_flag = F::from_bool(mulh_opcode == MulHOpcode::MULH);
        core_row.opcode_mulhsu_flag = F::from_bool(mulh_opcode == MulHOpcode::MULHSU);
        core_row.opcode_mulhu_flag = F::from_bool(mulh_opcode == MulHOpcode::MULHU);

        // TODO(ayush): move to fill_trace_row
        for i in 0..NUM_LIMBS {
            self.range_tuple_chip.add_count(&[a_mul[i], carry[i]]);
            self.range_tuple_chip
                .add_count(&[a[i], carry[NUM_LIMBS + i]]);
        }

        if mulh_opcode != MulHOpcode::MULHU {
            let b_sign_mask = if b_ext == 0 { 0 } else { 1 << (LIMB_BITS - 1) };
            let c_sign_mask = if c_ext == 0 { 0 } else { 1 << (LIMB_BITS - 1) };
            self.bitwise_lookup_chip.request_range(
                (b[NUM_LIMBS - 1] - b_sign_mask) << 1,
                (c[NUM_LIMBS - 1] - c_sign_mask) << ((mulh_opcode == MulHOpcode::MULH) as u32),
            );
        }

        // TODO(ayush): avoid this conversion
        let a = a.map(|x| x as u8);
        self.adapter
            .write(state.memory, instruction, adapter_row, &[a].into());

        *state.pc = state.pc.wrapping_add(DEFAULT_PC_STEP);

        *trace_offset += width;

        Ok(())
    }

    fn fill_trace_row(&self, mem_helper: &MemoryAuxColsFactory<F>, row_slice: &mut [F]) {
        let (adapter_row, _core_row) = unsafe { row_slice.split_at_mut_unchecked(A::WIDTH) };

        self.adapter.fill_trace_row(mem_helper, (), adapter_row);
    }
}

#[derive(AlignedBytesBorrow, Clone)]
#[repr(C)]
struct MulHPreCompute {
    local_opcode: MulHOpcode,
    a: u8,
    b: u8,
    c: u8,
}

impl<F, A, const LIMB_BITS: usize> StepExecutorE1<F>
    for MulHStep<A, { RV32_REGISTER_NUM_LIMBS }, LIMB_BITS>
where
    F: PrimeField32,
{
    #[inline(always)]
    fn execute_e1<Ctx>(&self) -> ExecuteFunc<F, Ctx>
    where
        Ctx: E1E2ExecutionCtx,
    {
        execute_e1_impl
    }

    // fn execute_metered(
    //     &self,
    //     state: &mut VmStateMut<F, GuestMemory, MeteredCtx>,
    //     instruction: &Instruction<F>,
    //     chip_index: usize,
    // ) -> Result<()> {
    //     self.execute_e1(state, instruction)?;
    //     state.ctx.trace_heights[chip_index] += 1;
    //
    //     Ok(())
    // }

    #[inline(always)]
    fn pre_compute_size(&self) -> usize {
        size_of::<MulHPreCompute>()
    }

    #[inline(always)]
    fn pre_compute(&self, inst: &Instruction<F>, data: &mut [u8]) {
        let pre_compute: &mut MulHPreCompute = data.borrow_mut();
        *pre_compute = MulHPreCompute {
            a: inst.a.as_canonical_u32() as u8,
            b: inst.b.as_canonical_u32() as u8,
            c: inst.c.as_canonical_u32() as u8,
            local_opcode: MulHOpcode::from_usize(
                inst.opcode.local_opcode_idx(MulHOpcode::CLASS_OFFSET),
            ),
        };
    }
}

unsafe fn execute_e1_impl<F: PrimeField32, CTX: E1E2ExecutionCtx>(
    inst: *const PreComputeInstruction<F, CTX>,
    vm_state: &mut VmSegmentState<F, CTX>,
) -> Result<()> {
    let next_inst = inst.offset(1);
    let curr_inst = &*inst;
    let pre_compute: &MulHPreCompute = curr_inst.pre_compute.borrow();

    let rs1: [u8; RV32_REGISTER_NUM_LIMBS] =
        vm_state.vm_read(RV32_REGISTER_AS, pre_compute.b as u32);
    let rs2: [u8; RV32_REGISTER_NUM_LIMBS] =
        vm_state.vm_read(RV32_REGISTER_AS, pre_compute.c as u32);
    let rd = match pre_compute.local_opcode {
        MulHOpcode::MULH => {
            let rs1 = i32::from_le_bytes(rs1) as i64;
            let rs2 = i32::from_le_bytes(rs2) as i64;
            ((rs1.wrapping_mul(rs2) >> 32) as u32).to_le_bytes()
        }
        MulHOpcode::MULHSU => {
            let rs1 = i32::from_le_bytes(rs1) as i64;
            let rs2 = u32::from_le_bytes(rs2) as i64;
            ((rs1.wrapping_mul(rs2) >> 32) as u32).to_le_bytes()
        }
        MulHOpcode::MULHU => {
            let rs1 = u32::from_le_bytes(rs1) as i64;
            let rs2 = u32::from_le_bytes(rs2) as i64;
            ((rs1.wrapping_mul(rs2) >> 32) as u32).to_le_bytes()
        }
    };
    vm_state.vm_write(RV32_REGISTER_AS, pre_compute.a as u32, &rd);

    vm_state.pc += DEFAULT_PC_STEP;
    vm_state.instret += 1;

    next_instruction!(next_inst, vm_state)
}

// returns mulh[[s]u], mul, carry, x_ext, y_ext
#[inline(always)]
pub(super) fn run_mulh<const NUM_LIMBS: usize, const LIMB_BITS: usize>(
    opcode: MulHOpcode,
    x: &[u32; NUM_LIMBS],
    y: &[u32; NUM_LIMBS],
) -> ([u32; NUM_LIMBS], [u32; NUM_LIMBS], Vec<u32>, u32, u32) {
    let mut mul = [0; NUM_LIMBS];
    let mut carry = vec![0; 2 * NUM_LIMBS];
    for i in 0..NUM_LIMBS {
        if i > 0 {
            mul[i] = carry[i - 1];
        }
        for j in 0..=i {
            mul[i] += x[j] * y[i - j];
        }
        carry[i] = mul[i] >> LIMB_BITS;
        mul[i] %= 1 << LIMB_BITS;
    }

    let x_ext = (x[NUM_LIMBS - 1] >> (LIMB_BITS - 1))
        * if opcode == MulHOpcode::MULHU {
            0
        } else {
            (1 << LIMB_BITS) - 1
        };
    let y_ext = (y[NUM_LIMBS - 1] >> (LIMB_BITS - 1))
        * if opcode == MulHOpcode::MULH {
            (1 << LIMB_BITS) - 1
        } else {
            0
        };

    let mut mulh = [0; NUM_LIMBS];
    let mut x_prefix = 0;
    let mut y_prefix = 0;

    for i in 0..NUM_LIMBS {
        x_prefix += x[i];
        y_prefix += y[i];
        mulh[i] = carry[NUM_LIMBS + i - 1] + x_prefix * y_ext + y_prefix * x_ext;
        for j in (i + 1)..NUM_LIMBS {
            mulh[i] += x[j] * y[NUM_LIMBS + i - j];
        }
        carry[NUM_LIMBS + i] = mulh[i] >> LIMB_BITS;
        mulh[i] %= 1 << LIMB_BITS;
    }

    (mulh, mul, carry, x_ext, y_ext)
}
