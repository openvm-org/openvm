name: "Benchmark: Coordinate Runner & Reporting"

on:
  workflow_dispatch:
    inputs:
      benchmark_name:
        type: choice
        required: true
        description: The name of the benchmark to run
        options:
          - vm_verify_fibair
          - halo2_static_verify_fibair
          - tiny_e2e
          - small_e2e
          - single_rw
          - single_filter
      aws_instance_type:
        type: string
        required: false
        description: The type of EC2 instance to start
        default: r7g.8xlarge
  workflow_call:
    inputs:
      benchmark_name:
        type: string
        required: true
        description: The name of the benchmark to run
      aws_instance_type:
        type: string
        required: false
        description: The type of EC2 instance to start
        default: r7g.8xlarge

env:
  S3_PATH: s3://axiom-workflow-data-staging-us-east-1/benchmark/github/results
  S3_METRICS_PATH: s3://axiom-workflow-data-staging-us-east-1/benchmark/github/metrics
  FEATURE_FLAGS: "bench-metrics,parallel,mimalloc"
  CMD_ARGS: ""

jobs:
  start-runner:
    name: Start self-hosted EC2 runner
    runs-on: [self-hosted, coordinator, staging]
    outputs:
      label: ${{ steps.start-ec2-runner.outputs.label }}
      ec2-instance-id: ${{ steps.start-ec2-runner.outputs.ec2-instance-id }}
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-region: us-east-1
      - name: Start EC2 runner
        id: start-ec2-runner
        uses: machulav/ec2-github-runner@v2
        with:
          mode: start
          github-token: ${{ secrets.GH_PERSONAL_ACCESS_TOKEN }}
          ec2-image-id: ami-0d33c5daee4966517 # arm64-rust-dev-32gb
          # ami-06dd8940c223fd887 # arm64-rust-dev-1024gb
          ec2-instance-type: ${{ inputs.aws_instance_type }}
          subnet-id: subnet-08094bf4edad62d10 # staging tooling private subnet
          security-group-id: sg-09543a06c701894f8 # allow all outbound
          iam-role-name: github-temp-runner # IAM role name to attach to the created EC2 runner

  bench-new:
    name: Run benchmark on workflow ref/branch
    needs:
      - start-runner
    runs-on: ${{ needs.start-runner.outputs.label }} # run the job on the newly created runner
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.head_ref || github.ref }}
      - uses: dtolnay/rust-toolchain@stable
      - uses: Swatinem/rust-cache@v2
        with:
          cache-on-failure: true

      - name: Setup halo2
        working-directory: recursion # We only ever run halo2 for recursion
        run: |
          if [[ "${{ github.event.pull_request.labels[*].name }}" == *"run-benchmark-e2e"* ]]; then
            echo "Adding static-verifier feature flag"
            CMD_ARGS="--features static-verifier"
            bash trusted_setup_s3.sh
          fi

      ######################################################
      # Run a different benchmark based on benchmark_name:
      - name: Run benchmark
        if: inputs.benchmark_name == 'vm_verify_fibair'
        working-directory: recursion
        run: |
          BIN_NAME="verify_fibair"
          python3 ../sdk/scripts/bench.py $BIN_NAME $CMD_ARGS
          echo "BIN_NAME=${BIN_NAME}" >> $GITHUB_ENV

      - name: Run benchmark
        if: inputs.benchmark_name == 'halo2_static_verify_fibair'
        working-directory: recursion
        run: |
          FEATURE_FLAGS="$FEATURE_FLAGS,static-verifier"
          bash trusted_setup_s3.sh
          mkdir -p ../benchmark/tmp
          RUSTFLAGS="-Ctarget-cpu=native" cargo test --release -p afs-recursion --test recursion --features $FEATURE_FLAGS -- test_fibonacci_program_halo2_verify > ../benchmark/tmp/_result.middle.md
          sed -n "/prove halo2 verifier circuit/,\$p" ../benchmark/tmp/_result.middle.md > ../benchmark/tmp/_result.md

      - name: Run benchmark
        if: inputs.benchmark_name == 'tiny_e2e'
        working-directory: recursion
        run: |
          BIN_NAME="tiny_e2e"
          python3 ../sdk/scripts/bench.py $BIN_NAME $CMD_ARGS
          echo "BIN_NAME=${BIN_NAME}" >> $GITHUB_ENV

      - name: Run benchmark
        if: inputs.benchmark_name == 'small_e2e'
        working-directory: recursion
        run: |
          BIN_NAME="small_e2e"
          python3 ../sdk/scripts/bench.py $BIN_NAME $CMD_ARGS
          echo "BIN_NAME=${BIN_NAME}" >> $GITHUB_ENV

      - name: Run benchmark
        if: inputs.benchmark_name == 'single_rw'
        run: |
          RUSTFLAGS="-Ctarget-cpu=native" cargo run --release --bin benchmark --features parallel -- rw -r 90 -w 10 --config-folder benchmark/config/single_rw

      - name: Run benchmark
        if: inputs.benchmark_name == 'single_filter'
        run: |
          RUSTFLAGS="-Ctarget-cpu=native" cargo run --release --bin benchmark --features parallel -- predicate --config-folder benchmark/config/single_filter -f benchmark/config/olap/filter_0xfade.afo
      ######################################################
      - name: Store metric json and compute diff with previous
        run: |
          METRIC_PATH=".bench_metrics/${BIN_NAME}.json"
          echo "METRIC_PATH=${METRIC_PATH}" >> $GITHUB_ENV

          current_sha=$(git rev-parse HEAD)
          echo "Current SHA: $current_sha"
          echo "current_sha=${current_sha}" >> $GITHUB_ENV

          if [[ -f $METRIC_PATH ]]; then
            s5cmd cp $METRIC_PATH ${{ env.S3_METRICS_PATH }}/${current_sha}-${{ inputs.benchmark_name }}.json

            prev_path="${{ env.S3_METRICS_PATH }}/main-${{ inputs.benchmark_name }}.json"
            count=`s5cmd ls $prev_path | wc -l`

            if [[ $count -gt 0 ]]; then
              s5cmd cp $prev_path prev.json
              python3 sdk/scripts/metric_unify/main.py $METRIC_PATH --prev prev.json > results.md
            else
              echo "No previous benchmark on main branch found"
              python3 sdk/scripts/metric_unify/main.py $METRIC_PATH > results.md
            fi
          else
            echo "No benchmark metrics found at ${METRIC_PATH}"
            # Old fallback until we switch all benchmarks to new metrics format:
            cp benchmark/tmp/_result.md results.md
          fi

      - name: Install inferno-flamegraph
        run: cargo install inferno

      - name: Generate flamegraphs
        run: |
          if [[ -f $METRIC_PATH ]]; then
            python3 sdk/scripts/metric_unify/flamegraph.py $METRIC_PATH
            FLAMEGRAPH_PREFIX=".bench_metrics/flamegraphs/${BIN_NAME}"
            echo "UPLOAD_FLAMEGRAPHS=1" >> $GITHUB_ENV
          fi

      - name: Upload flamegraphs
        id: flamegraphs-upload-step
        if: env.UPLOAD_FLAMEGRAPHS == '1'
        uses: actions/upload-artifact@v4
        with:
          name: flamegraphs
          path: .bench_metrics/flamegraphs/*.svg
          retention-days: 7
          overwrite: true

      - name: Add benchmark metadata
        run: |
          commit_url="https://github.com/${{ github.repository }}/commit/${current_sha}"
          RESULT_PATH=results.md
          echo "" >> $RESULT_PATH
          if [[ "$UPLOAD_FLAMEGRAPHS" == '1' ]]; then
            echo "Flamegraphs: [link](${{ steps.flamegraphs-upload-step.outputs.artifact-url }})" >> $RESULT_PATH
          fi
          echo "Commit: ${commit_url}" >> $RESULT_PATH
          echo "AWS Instance Type: [${{ inputs.aws_instance_type }}](https://instances.vantage.sh/aws/ec2/${{ inputs.aws_instance_type }})" >> $RESULT_PATH
          echo "[Benchmark Workflow](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $RESULT_PATH
          s5cmd cp $RESULT_PATH "${{ env.S3_PATH }}/${current_sha}-${{ inputs.benchmark_name }}.md"

      - name: Collapse previous comment (if exists)
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const comments = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number
            });
            for (const comment of comments.data) {
              if (comment.user.login == "github-actions[bot]" && comment.body.startsWith("${{ inputs.benchmark_name }}")) {
                console.log("collapse comment ", comment.id);
                const resp = await github.graphql(`
                  mutation {
                    minimizeComment(input: {classifier: OUTDATED, subjectId: "${comment.node_id}"}) {
                      minimizedComment {
                        isMinimized
                      }
                    }
                  }
                `);
              }
            }

      - name: Add comment to pull request
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs')
            const newBenchmark = fs.readFileSync('./results.md', { encoding: 'utf8', flag: 'r' })

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `${{ inputs.benchmark_name }}\n\n${newBenchmark}`
            });

      - name: Update latest main result in s3
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          s5cmd cp "${{ env.S3_PATH }}/${{ env.current_sha }}-${{ inputs.benchmark_name }}.md" "${{ env.S3_PATH }}/main-${{ inputs.benchmark_name }}.md"

          if [[ -f $METRIC_PATH ]]; then
            s5cmd cp $METRIC_PATH "${{ env.S3_METRICS_PATH }}/main-${{ inputs.benchmark_name }}.json"
          fi

  update-gh-pages:
    name: Update github pages with new bench results
    needs:
      - start-runner
      - bench-new
    runs-on: ${{ needs.start-runner.outputs.label }} # run the job on the newly created runner
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    steps:
      - uses: actions/checkout@v4
        with:
          ref: gh-pages

      - name: Update markdown file
        run: |
          s5cmd cp "${{ env.S3_PATH }}/main-${{ inputs.benchmark_name }}.md" benchmarks/${{ inputs.benchmark_name }}.md
          git add benchmarks/${{ inputs.benchmark_name }}.md
          git commit -m "Update benchmark result for ${{ inputs.benchmark_name }}"
          git push

  stop-runner:
    name: Stop self-hosted EC2 runner
    needs:
      - start-runner # required to get output from the start-runner job
      - bench-new # wait for bench-new to finish
      - update-gh-pages # wait for update-gh-pages to finish
    runs-on: [self-hosted, coordinator, staging]
    if: ${{ always() }} # required to stop the runner even if the error happened in the previous jobs
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-region: us-east-1
      - name: Stop EC2 runner
        uses: machulav/ec2-github-runner@v2
        with:
          mode: stop
          github-token: ${{ secrets.GH_PERSONAL_ACCESS_TOKEN }}
          label: ${{ needs.start-runner.outputs.label }}
          ec2-instance-id: ${{ needs.start-runner.outputs.ec2-instance-id }}
